# Official language image. Look for the different tagged releases at:
# https://hub.docker.com/r/library/python/tags/
image: python:3.9

# Change pip's cache directory to be inside the project directory since we can
# only cache local items.
variables:
  PIP_CACHE_DIR: "$CI_PROJECT_DIR/.cache/pip"

# Pip's cache doesn't store the python packages
# https://pip.pypa.io/en/stable/topics/caching/
#
# If you want to also cache the installed packages, you have to install
# them in a virtualenv and cache it as well.
cache:
  paths:
    - .cache/pip
    - venv/

# There are two stages in this pipeline: unittest and workflow
stages:
  - unittest       # For simple unit tests
  - workflow       # For larger flows of the package
  - documentation  # For generating and checking the documentation

# A template to run the different  unittests  in the different subpackages in separate jobs
.template_unit_test:
  variables:
    DIRECTORY: ""  # A variable to indicate which subpackage to test
  stage: unittest  # Run in `unittest`stage
  before_script:
    - python --version ; pip --version  # For debugging
    - pip install -r requirements.txt   # The requirements to run the code
    - pip install pytest pytest-cov     # Packages for testing purposes
  script:
    # Test the requested package, print the code coverage, and generate a xml report of the tests
    - pytest ./tests/$DIRECTORY --cov=dtaianomaly/$DIRECTORY --cov-report term-missing --junitxml=report.xml -v
  coverage: '/(?i)TOTAL.*? (100(?:\.0+)?\%|[1-9]?\d(?:\.\d+)?\%)$/'  # Extract the  code coverage
  artifacts:
    when: always  # Always savee the urest report, and let it expire after a week
    reports:
      junit: report.xml
    expire_in: 1 weeks

test-anomaly-detection:
  extends: .template_unit_test
  variables:
    DIRECTORY: anomaly_detection

test-data-management:
  extends: .template_unit_test
  variables:
    DIRECTORY: data_management

test-evaluation-detection:
  extends: .template_unit_test
  variables:
    DIRECTORY: evaluation 

# Check if the main script still executes without any problems
run-main-script:
  stage: workflow
  before_script:
    - python --version ; pip --version
    - pip install -r requirements.txt
  script:
    - python main.py  
          --data_dir data
          --datasets_index_file datasets.csv
          --configuration_dir configurations
          --data data/Demo.json
          --algorithm algorithm/iforest.json
          --metric metric/auc_metrics.json
          --output output/output_all.json

# Check if the notebooks still run without any problems
run-notebooks:
  stage: workflow
  before_script:
    - python --version ; pip --version
    - pip install -r requirements.txt
    - pip install jupyter
  script:
    - mv dtaianomaly notebooks
    - cd notebooks
    - jupyter nbconvert --to notebook --execute *.ipynb

doctest:
  stage: documentation
  image: python:3.9-slim
  before_script:
    - apt-get update && apt-get install make --no-install-recommends -y
    - python -m pip install sphinx furo
  script:
    - cd docs && make doctest

docgen:
  stage: documentation
  image: python:3.9-slim
  before_script:
    - apt-get update && apt-get install make --no-install-recommends -y
    - python -m pip install sphinx furo
  script:
    - cd docs && make html
  after_script:
    - mv docs/_build/html/ ./public/
  artifacts:
    paths:
    - public
  rules:
    - if: $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH