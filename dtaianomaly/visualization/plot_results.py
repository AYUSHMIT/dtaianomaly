
import os
from typing import Dict, Any, List, Optional, Tuple
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from dtaianomaly.visualization.critical_difference_diagram import wilcoxon_holm, graph_ranks


def read_results(directory: str,
                 map_sub_dir_name: Optional[Dict[str, Any]] = None,
                 filter_sub_dirs: Optional[List[str]] = None,
                 results_file_name: str = 'results',
                 results_must_match: bool = True) -> pd.DataFrame:
    """
    Read the results from the given directory and format them, such that the resulting
    dataframe can be used in the other methods to plot the results.

    Parameters
    ----------
    directory : str
        The directory in which the results are. These results should be formatted
        according to how the workflow writes the results. That is, the method will
        search each subdirectory in the given directory (not recursively) and read
        the data in the results file.
    map_sub_dir_name : Dict[str, Any], optional, default=None
        A dictionary which is used to automatically map the name of the
        subdirectory to a value. This is useful in case the subdirectory
        name contains some additional information (e.g., hyperparameter
        values).
    filter_sub_dirs : List[str], optional, default=None
        The subdirectories that should not be included in the resulting dataframe.
    results_file_name : str, optional, default='results'
        The name of the results file within each subdirectory. Note that this
        should be a csv file, and that the extension '.csv' should not be
        included in the given value.
    results_must_match : bool, default=True
        Whether the results in the different result files should contain exactly the
        same instances and columns. If True, then an error will be raised if the data
        does not match. If False, no error will be thrown and the intersection will
        be taken.

    Raises
    ------
    Exception
        If ``results_must_match == True`` and the results do not actually match
        (i.e., have the same information about the same instances).

    Returns
    -------
    plt.Figure
        A single pandas dataframe that contains all the information in the results file.
    """
    # Read the results
    results = []
    for sub_dir_name in os.listdir(directory):
        if filter_sub_dirs and sub_dir_name in filter_sub_dirs:
            continue
        algorithm_results = pd.read_csv(f'{directory}/{sub_dir_name}/{results_file_name}.csv')
        if map_sub_dir_name is not None:
            for column_name, mapping in map_sub_dir_name.items():
                algorithm_results[column_name] = mapping(sub_dir_name)
        results.append(algorithm_results)

    # check if the results are about the same datasets
    df_to_match = results[0]
    joint_collections = set((df_to_match['collection_name'] + results[0]['dataset_name']).values)
    for other_df in results[1:]:
        collections_other = set((other_df['collection_name'] + other_df['dataset_name']).values)
        collections_match = (collections_other == joint_collections)
        columns_match = set(df_to_match.columns) == set(other_df.columns)
        if not (collections_match and columns_match) and results_must_match:
            raise Exception('The results are not about exactly the same data!')
        joint_collections &= collections_other
    filtered_dfs = [df[(df['collection_name'] + df['dataset_name']).isin(joint_collections)] for df in results]

    # Concat and return the data
    return pd.concat(filtered_dfs, axis=0, join='inner')


def plot_performance(
        data: pd.DataFrame,
        x_value: str,
        values_of_interest: List[str],
        type_of_plot: str = 'boxplot',
        rename_x_values: Dict[str, str] = None,
        rename_values_of_interest: Dict[str, str] = None,
        sort_results: bool = True,
        fig_size: Tuple[int, int] = (20, 4),
        title: str = None,
        x_label: str = None,
        y_label: str = None,
        y_log_scale: bool = False,
        file_path: Optional[str] = None) -> plt.Figure:
    """
    Plots the performance of the anomaly detectors, which is provided in the given data frame.

    Parameters
    ----------
    data : pd.DataFrame
        The evaluation matrix of all methods, as generated by :py:func:`~dtaianomaly.visualization.read_results`.
    x_value : str
        The column of the data frame group the performance by and put on the x-axis of the plot.
    values_of_interest : List[str]
        The columns of the data frame that should be shown. For every value in ``x_value`` column, the
        aggregated performance for each columns in ``values_of_interest`` will be shown.
    type_of_plot : str, default='boxplot'
        How to show the results. Can be either ``'boxplot'`` (default) or ``'bar'``.
    rename_x_values : Dict[str, str], optional, default=None
        A dictionary representing how to map the names of the values on the x-axis. Can be used
        for more informative names besides what is available in the data frame. Names for which
        no key in the given dictionary exists will not be changed. If None, no name will be
        adjusted.
    rename_values_of_interest : Dict[str, str], optional, default=None
        A dictionary representing how to map the names of the performances measures, to
        provide more informative names. Names for which no key in the given dictionary
        exists will not be changed. By default, no name will be adjusted.
    sort_results : bool, default=True
        Whether the values on the x-axis should be sorted or not.
    fig_size : Tuple[int, int], default=(20, 4)
        The size of the figure as (``width``, ``height``).
    title : str, optional, default=None
        A title to add to the plot. If None, no title will be added.
    x_label : str, optional, default=None
        A custom label for the x-axis of the figure. If None, ``x_value`` will be on the x-axis.
    y_label : str, optional, default=None
        A custom label for the y-axis of the figure. If None, no label will be added.
    y_log_scale : bool, default=False
        Whether the y-axis should be in logarithmic scale. If True and a title should be added to
        the figure (through the ``title`` parameter), then a note ``'(log scale)'`` will be added to
        the title.
    file_path : str, optional, default=None
        If provided, the resulting figure will be saved to this path. Otherwise, the
        file will not be saved.

    Raises
    ------
    ValueError
        If ``type_of_plot`` is not a valid value.

    Returns
    -------
    plt.Figure
        The figure containing the plotted performances.
    """
    # Format the data
    df_melted = data[[x_value] + values_of_interest].melt(id_vars=x_value, var_name='Measure', value_name='Score')
    if rename_x_values:
        df_melted[x_value] = df_melted[x_value].replace(rename_x_values)
    if rename_values_of_interest:
        df_melted['Measure'] = df_melted['Measure'].replace(rename_values_of_interest)

    # Sort the data, if requested
    order = df_melted.groupby(x_value)['Score'].mean().sort_values().index if sort_results else df_melted.groupby(x_value)['Score'].mean().index

    # Plot the data
    plt.figure(figsize=fig_size)
    if type_of_plot == 'boxplot':
        ax = sns.boxplot(data=df_melted, x=x_value, y='Score', hue='Measure', order=order)
    elif type_of_plot == 'bar':
        ax = sns.barplot(data=df_melted, x=x_value, y='Score', hue='Measure', order=order)
    else:
        raise ValueError(f"Invalid type of plot given: '{type_of_plot}'! Only 'boxplot' and 'bar' are allowed!")

    # Format the axis
    ax.legend_.set_title('')
    ax.set_title(title)
    ax.set_ylabel(y_label)
    ax.set_xlabel(x_label)
    if y_log_scale:
        ax.semilogy()
        if title is not None:
            ax.set_title(title + ' (log scale)')

    # Obtain the figure
    fig = ax.get_figure()

    # Save the figure if requested
    if file_path is not None:
        fig.savefig(file_path)

    return fig


def plot_ranking(
        data: pd.DataFrame,
        x_value: str,
        values_of_interest: List[str],
        smaller_is_better: bool = False,
        **kwargs) -> plt.Figure:
    """
    Rank the algorithms according to the given performances, which is provided in the given data frame.

    Parameters
    ----------
    data : pd.DataFrame
        The evaluation matrix of all methods, as generated by :py:func:`~dtaianomaly.visualization.read_results`.
    x_value : str
        The column of the data frame group the performance by and put on the x-axis of the plot.
    values_of_interest : List[str]
        The columns of the data frame that should be shown. For every value in ``x_value`` column, the
        aggregated performance for each columns in ``values_of_interest`` will be shown.
    smaller_is_better : bool, default=False
        Whether a higher performance value is better.
    kwargs :
        Additional parameters to pass to the :py:func:`~dtaianomaly.visualization.plot_performance` function.

    Returns
    -------
    plt.Figure
        The figure containing the plotted ranking.
    """
    # Compute the rank of each time series on each instance
    group = ['collection_name', 'dataset_name'] + [x_value]
    df_evaluation_metric = data[group + values_of_interest].groupby(group).mean()
    df_evaluation_metric = df_evaluation_metric.groupby(['collection_name', 'dataset_name']).rank(method='average', ascending=smaller_is_better)

    # Show the ranking-performance
    return plot_performance(
        data=df_evaluation_metric.reset_index(level=x_value),
        x_value=x_value,
        values_of_interest=values_of_interest,
        **kwargs
    )


def plot_critical_difference_diagram(
            data: pd.DataFrame,
            group_measurements_by: str,
            score: str,
            smaller_is_better: bool = False,
            alpha: float = 0.05,
            fig_size: Tuple[int, int] = (20, 4),
            title: str = None,
            file_path: Optional[str] = None,
            show_rank: bool = True,
            width: float = 9.0,
            text_space: float = 1.0,
            big_tick_length: float = 0.3,
            small_tick_length: float = 0.15,
            linewidth: float = 2.0,
            linewidth_significance_bar: float = 4.0,
            space_between_significance_bars: float = 0.1) -> plt.Figure:
    """
    Plot a critical difference [demsar2006statistical]_ of the algorithms in the given dataframe.

    Parameters
    ----------
    data : pd.DataFrame
        The evaluation matrix of all methods, as generated by :py:func:`~dtaianomaly.visualization.read_results`.
    group_measurements_by : str
        The column of the data frame to group the performance by and show as a separate instance in
        the critical difference diagram.
    score : str
        The score to use, according to which the various groups are ranked
    smaller_is_better : bool, default=False
        Whether a lower score is better.
    alpha : float, default=0.05
        The significance level used for computing the bars in the critical difference diagram.
    fig_size : Tuple[int, int], default=(20, 4)
        The size of the figure as (``width``, ``height``).
    title : str, optional, default=None
        A title to add to the plot.
    file_path : str, optional, default=None
        If provided, the resulting figure will be saved to this path. Otherwise, the
        file will not be saved.
    show_rank : bool, default=True
        Whether the average rank should be plotted on the diagram.
    width : float, default=9.0
        The length of the ranking axis. Note that this is different from the size of
        the figure (which can be configured through ``fig_size``). It dictates the
        length of the ranking axis in comparison to the remainder of the figure.
    text_space : float, default=1.0
        The amount of space at the side of the figure reserved for plotting the labels
        of the different instances.
    big_tick_length : float, default=0.3
        How large the big ticks at the ranking-axis should be, that is, the ticks that
        are located at the integer values of the ranks.
    small_tick_length : float, default=0.15
        How large the small ticks at the ranking-axis should be, that is, the ticks that
        are located in the middle between two integer values of the ranks.
    linewidth : float, default=2.0
        How tick the lines of the ranking-axis and the lines directing the instances
        to their rank should be.
    linewidth_significance_bar : float, default=4.0
        How thick the significance bars should be.
    space_between_significance_bars : float, default=0.1
        The spacing between two significance bars.

    Returns
    -------
    plt.Figure
        The figure containing the plotted critical difference diagram.

    Reference
    ---------
    .. [demsar2006statistical] J. Demšar, “Statistical Comparisons of Classifiers over Multiple Data Sets,” J. Mach. Learn. Res., vol.
                               7, pp. 1–30, Dec. 2006. `10.5555/1248547.1248548 <https://dl.acm.org/doi/10.5555/1248547.1248548>`_
    """

    data_subset = data[[group_measurements_by, score]]
    data_subset.insert(0, 'dataset_name', data["collection_name"] + data["dataset_name"])

    data_subset = data_subset.rename(columns={
        group_measurements_by: 'classifier_name',
        score: 'accuracy'
    })
    if smaller_is_better:
        data_subset['accuracy'] = -data_subset['accuracy']  # For the call, higher should be better

    data_subset = data_subset.groupby(['dataset_name', 'classifier_name']).mean().reset_index()

    fig = plt.figure(figsize=fig_size)

    p_values, average_ranks, _ = wilcoxon_holm(df_perf=data_subset, alpha=alpha)
    graph_ranks(
        fig,
        avranks=average_ranks.values,
        names=average_ranks.keys(),
        p_values=p_values,
        reverse=True,
        show_rank=show_rank,
        width=width,
        textspace=text_space,
        bigtick=big_tick_length,
        smalltick=small_tick_length,
        linewidth=linewidth,
        linewidth_significance=linewidth_significance_bar,
        space_between_bars=space_between_significance_bars
    )

    fig.suptitle(title)

    # Save the figure if requested
    if file_path is not None:
        fig.savefig(file_path)

    return fig


def main():

    results = read_results(
        '../../large_results',
        map_sub_dir_name={
            'Raw anomaly detector': lambda sub_dir: sub_dir,
            'Anomaly detector': lambda sub_dir: sub_dir.split('_')[0],
            'Window size': lambda sub_dir: int(sub_dir.split('_')[1])
        }
    )

    plot_performance(
        results,
        x_value='Raw anomaly detector',
        values_of_interest=['roc_vus', 'pr_vus']
    ).show()

    plot_ranking(
        results,
        x_value='Raw anomaly detector',
        values_of_interest=['roc_vus', 'pr_vus']
    ).show()

    plot_critical_difference_diagram(
        results,
        group_measurements_by='Anomaly detector',
        score='roc_vus',
        title=f'Critical difference',
        file_path='cd_diagram.png',
        fig_size=(20, 2)
    ).show()


if __name__ == '__main__':
    main()
